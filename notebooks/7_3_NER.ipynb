{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Named Entity Recognition in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Konstantin Todorov\n",
    "\n",
    "Goals of this session:\n",
    "* Learn how to use advanced Recurrent Neural Networks (RNN)\n",
    "* Learn how to perform token classification in a more complex environment such as the Named Entity Recognition (NER) task\n",
    "* Learn which hyper-parameters are important when training on a multi-classification problem\n",
    "* Learn to use the HuggingFace library for declaring Transformer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the problem of [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (NER) tagging of sentences. The task is to tag each token in a given sentence with an appropriate label such as Person, Location, etc. Further more, the tags can start with either 'B-' or 'I-', meaning beginning or inside respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with including the necessary libraries, setting the torch device that will be used and ensure we have reproducibility by setting a seed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \\'{device}\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set a seed manually for reproducing purposes. \n",
    "# This will ensure that every time you run the notebook on the same machine you will receive the same results\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have ensured the basics, we must download the dataset we will be using. \n",
    "\n",
    "Below you will see a function which downloads [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) shared task dataset and stores it locally for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_conll_2003_data():\n",
    "    data_folder = '.data'\n",
    "    if os.path.exists(data_folder) and len(os.listdir(data_folder)) > 3:\n",
    "        print('CoNLL-2003 data already downloaded')\n",
    "        return\n",
    "    \n",
    "    url = 'https://data.deepai.org/conll2003.zip'\n",
    "    filename, _ = urllib.request.urlretrieve(url)\n",
    "    if filename is None:\n",
    "        raise Exception('Could not download CoNLL-2003 data. Please check your internet connection')\n",
    "    \n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_folder)\n",
    "        \n",
    "    print('Data downloaded successfully')\n",
    "    \n",
    "download_conll_2003_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is downloaded, we need to define some functions to process it.\n",
    "\n",
    "Firstly, we will define a function that given a file from the corpus, it reads it line by line and returns a collection of each word and its correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(corpus_file):\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        examples = []\n",
    "        doc_words = []\n",
    "        doc_labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                examples.append((doc_words, doc_labels))\n",
    "\n",
    "                doc_words = []\n",
    "                doc_labels = []\n",
    "            else:\n",
    "                columns = line.split()                \n",
    "                doc_words.append(columns[0])\n",
    "                doc_labels.append(columns[-1])\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we must also build our vocabularies. Similarly to the previous lab, we will be using TorchText and the `Vocab` object that it provides. This time, the difference will be that we must also build a vocabulary for our labels as we are now performing multi-classification instead of a binary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocabularies(train_data, vectors=None):\n",
    "    counter = Counter()\n",
    "    label_counter = Counter()\n",
    "    for (doc_words, doc_labels) in train_data:\n",
    "        counter.update(doc_words)\n",
    "        label_counter.update(doc_labels)\n",
    "\n",
    "    vocab = Vocab(counter, min_freq=1, vectors=vectors)\n",
    "    label_vocab = Vocab(label_counter, min_freq=1)\n",
    "    \n",
    "    return vocab, label_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine the defined functions to build data on which we can train and validate our models. In addition, we also get two vocabularies, one for the words and another for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(use_pretrained = False, lower=False):\n",
    "    # Read training and validation data according to the predefined split.\n",
    "    train_examples = read_data('.data/train.txt')\n",
    "    valid_examples = read_data('.data/valid.txt')\n",
    "    test_examples = read_data('.data/test.txt')\n",
    "\n",
    "    vectors = None\n",
    "    if use_pretrained:\n",
    "        vectors=\"glove.6B.300d\"\n",
    "    \n",
    "    vocab, label_vocab = build_vocabularies(train_examples, vectors)\n",
    "    \n",
    "    return vocab, label_vocab, train_examples, valid_examples, test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, label_vocab, train_examples, valid_examples, test_examples = build_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate our vocabularies again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique tokens we have in our vocabularies\n",
    "\n",
    "print(f\"Unique tokens in vocabulary: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also check the most common tokens in our vocabulary\n",
    "\n",
    "print(vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see the unique tokens in our label vocabulary\n",
    "\n",
    "print(f\"Tokens in the label vocabulary: {label_vocab.freqs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the final steps in pre-processing is to convert the raw texts we have to their integer versions. For each pair of word-label we convert it to their respective ids in the word and label vocabularies. \n",
    "\n",
    "We do this for the training, as well as for the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_tokens(examples, vocab, label_vocab):\n",
    "    result = []\n",
    "    for doc_words, doc_labels in examples:\n",
    "        word_tokens = [vocab[word] for word in doc_words]\n",
    "        label_tokens = [label_vocab[label] for label in doc_labels]\n",
    "        result.append((word_tokens, label_tokens))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = convert_examples_to_tokens(train_examples, vocab, label_vocab)\n",
    "valid_tokens = convert_examples_to_tokens(valid_examples, vocab, label_vocab)\n",
    "test_tokens = convert_examples_to_tokens(test_examples, vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data ready for use, we must make one final adjustment. At every step during training, we must organize our data neatly in a batch. If you recall from last exercise, we use PyTorch's data loaders and specifically their `collate_fn` argument. \n",
    "\n",
    "Below, we define a function `collate_batch` which accepts the batch and information about the padding and transforms and sorts it before returning the sequences, labels and lengths of sequences as tensors in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch, label_pad_idx=1):\n",
    "    batch_size = len(batch)\n",
    "    word_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # Tokenize labels and texts\n",
    "    for (word_tokens, label_tokens) in batch:\n",
    "        word_list.append(word_tokens)\n",
    "        label_list.append(label_tokens)\n",
    "\n",
    "    # Put all texts into a single numpy array of uniform length\n",
    "    # For all sequences that are shorther than the maximum length, pad to the right with 0\n",
    "    lengths = [len(words) for words in word_list]\n",
    "    max_length = max(lengths)\n",
    "    padded_sequences = np.zeros((batch_size, max_length), dtype=np.int64)\n",
    "    padded_labels = np.zeros((batch_size, max_length), dtype=np.int64)\n",
    "    padded_labels.fill(label_pad_idx)\n",
    "\n",
    "    for i, length in enumerate(lengths):\n",
    "        padded_sequences[i][0:length] = word_list[i][0:length]\n",
    "        padded_labels[i][0:length] = label_list[i][0:length]\n",
    "        \n",
    "    # Finally transform the arrays into tensors and return to the dataloader\n",
    "    sequences_tensor = torch.from_numpy(padded_sequences).to(device)\n",
    "    label_tensor = torch.from_numpy(padded_labels).to(device)\n",
    "    \n",
    "    # Sort the tensors by length\n",
    "    seq_lengths, perm_idx = torch.tensor(lengths).sort(0, descending=True)\n",
    "    sequences_tensor = sequences_tensor[perm_idx]\n",
    "    label_tensor = label_tensor[perm_idx]\n",
    "    return sequences_tensor, label_tensor, seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to define the dataloaders we will be using throughout the training, we create the `get_dataloaders` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_tokens, valid_tokens, test_tokens, pad_idx, label_pad_idx, batch_size):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_tokens,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_batch)\n",
    "    \n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_tokens,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_batch)\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_tokens,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_batch)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "pad_idx = vocab['<pad>']\n",
    "label_pad_idx = label_vocab['<pad>']\n",
    "\n",
    "(train_dataloader, valid_dataloader, test_dataloader) = get_dataloaders(\n",
    "    train_tokens, valid_tokens, test_tokens, pad_idx, label_pad_idx, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of NER, we see that the amount of non tagged words compared to our other tags is much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tagged_percentage = (label_vocab.freqs['O'] / sum(label_vocab.freqs.values())) * 100\n",
    "\n",
    "print(f'Percentage of non tagged words: {non_tagged_percentage:.4}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that metrics that we usually use such as accurracy will not work here. If we do use it, our model learning to predict **only** 'O' will easily achieve accurracy of at least the value above. We must reside to another metric, namely the F1 score which takes into account the precision and recall of the model predictions and does not allow for a single tag to hide the rest.\n",
    "\n",
    "Using the F1 score requires us to define multiple functions that are required to evaluate our predictions and compare them to the ground truth labels. Below, you will see the definitions functions, commented on how they are working exactly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a list of BIO labels, coded as integers, into spans identified by a beginning, an end, and a label.\n",
    "# To allow easy comparison later, we store them in a dictionary indexed by the start position.\n",
    "def to_spans(l_ids, voc):\n",
    "    spans = {}\n",
    "    current_lbl = None\n",
    "    current_start = None\n",
    "    \n",
    "    for i, l_id in enumerate(l_ids):\n",
    "        l = voc[l_id]\n",
    "\n",
    "        if l[0] == 'B': \n",
    "            # Beginning of a named entity: B-something.\n",
    "            if current_lbl:\n",
    "                # If we're working on an entity, close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "                \n",
    "            # Create a new entity that starts here.\n",
    "            current_lbl = l[2:]\n",
    "            current_start = i\n",
    "        elif l[0] == 'I':\n",
    "            # Continuation of an entity: I-something.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, but its label does not\n",
    "                # correspond to the predicted I-tag, then we close\n",
    "                # the open entity and create a new one.\n",
    "                if current_lbl != l[2:]:\n",
    "                    spans[current_start] = (current_lbl, i)\n",
    "                    current_lbl = l[2:]\n",
    "                    current_start = i\n",
    "            else:\n",
    "                # If we don't have an open entity but predict an I tag,\n",
    "                # we create a new entity starting here even though we're\n",
    "                # not following the format strictly.\n",
    "                current_lbl = l[2:]\n",
    "                current_start = i\n",
    "        else:\n",
    "            # Outside: O.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, we close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "                current_lbl = None\n",
    "                current_start = None\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares two sets of spans and records the results for future aggregation\n",
    "def compare(gold, pred, stats):\n",
    "    for start, (lbl, end) in gold.items():\n",
    "        stats['total']['gold'] += 1\n",
    "        stats[lbl]['gold'] += 1\n",
    "        \n",
    "    for start, (lbl, end) in pred.items():\n",
    "        stats['total']['pred'] += 1\n",
    "        stats[lbl]['pred'] += 1\n",
    "        \n",
    "    for start, (glbl, gend) in gold.items():\n",
    "        if start in pred:\n",
    "            plbl, pend = pred[start]\n",
    "            if glbl == plbl and gend == pend:\n",
    "                stats['total']['corr'] += 1\n",
    "                stats[glbl]['corr'] += 1\n",
    "\n",
    "# Computes precision, recall and F-score, given a dictionary that contains\n",
    "# the counts of correct, predicted and gold-standard items\n",
    "def prf(stats):\n",
    "    if stats['pred'] == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    p = stats['corr']/stats['pred']\n",
    "    r = stats['corr']/stats['gold']\n",
    "    if p > 0 and r > 0:\n",
    "        f = 2*p*r/(p+r)\n",
    "    else:\n",
    "        f = 0\n",
    "    return p, r, f\n",
    "\n",
    "# This function combines the auxiliary functions we defined above.\n",
    "def evaluate_iob(predicted, gold, label_vocab, stats):\n",
    "    # The gold-standard labels are assumed to be an integer tensor of shape\n",
    "    # [batch_size x max_length], as returned by the dataloader.\n",
    "    gold_cpu = gold.cpu().numpy()\n",
    "    gold_cpu = list(gold_cpu.reshape(-1))\n",
    "\n",
    "    # We just flatten the list.\n",
    "    pred_cpu = [l for sen in predicted for l in sen]\n",
    "    \n",
    "    # Compute spans for the gold standard and prediction.\n",
    "    gold_spans = to_spans(gold_cpu, label_vocab.itos)\n",
    "    pred_spans = to_spans(pred_cpu, label_vocab.itos)\n",
    "\n",
    "    # Finally, update the counts for correct, predicted and gold-standard spans.\n",
    "    compare(gold_spans, pred_spans, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our tagger model. Below, you will see the declaration of `RNNTagger`. \n",
    "\n",
    "There are multiple interesting points. For layers, we make use of the embedding layer we learned before, as well as an RNN layer. In the end, we end up with a linear layer which maps the dimension down to the desired output.\n",
    "\n",
    "The RNN layer is custom and is defined based on an argument. We define two types - the most simple one - a basic Recurrent neural network (RNN) and one that has become increasingly more popular in the recent years - Gated Recurrent Unit (GRU). \n",
    "\n",
    "Besides the required `.forward()` definition, we define `.predict()` function which we will use for evaluation over the trained model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        output_dim,\n",
    "        pad_word_idx,\n",
    "        pad_label_idx,\n",
    "        n_layers=1,\n",
    "        bidirectional=True,\n",
    "        rnn_type='rnn',\n",
    "        pretrained_weights=None,\n",
    "        update_pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        # If we're using pre-trained embeddings, copy them into our embedding module.\n",
    "        self._embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_word_idx)\n",
    "        if pretrained_weights is not None:\n",
    "            self._embedding_layer.weight = torch.nn.Parameter(\n",
    "                pretrained_weights,\n",
    "                requires_grad=update_pretrained)\n",
    "\n",
    "        # RNN layer\n",
    "        # This depends on the argument passed. We can use RNN, GRU or LSTM\n",
    "        if rnn_type == 'rnn':\n",
    "            self._rnn_layer = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_size,\n",
    "                bidirectional=bidirectional,\n",
    "                num_layers=n_layers)\n",
    "        elif rnn_type == 'gru':\n",
    "            self._rnn_layer = nn.GRU(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_size,\n",
    "                bidirectional=bidirectional,\n",
    "                num_layers=n_layers)\n",
    "        else:\n",
    "            raise Exception('Unsupported RNN type')\n",
    "\n",
    "        rnn_output_size = hidden_size\n",
    "        if bidirectional:\n",
    "            rnn_output_size = rnn_output_size * 2\n",
    "            \n",
    "        # Output layer\n",
    "        # The input of this layer is the output of the RNN layer\n",
    "        # Since we are always using bi-directional RNN, we multiply the RNN hidden size by 2\n",
    "        self._fc_layer = nn.Linear(rnn_output_size, output_dim)\n",
    " \n",
    "        # To deal with the padding positions later, we need to know the\n",
    "        # ids of the word pad tokens and the label pad tokens\n",
    "        self._pad_word_idx = pad_word_idx\n",
    "        self._pad_label_idx = pad_label_idx\n",
    "          \n",
    "    def forward(self, sentences, lengths):\n",
    "        # The words in the documents are encoded as integers. The shape of the documents\n",
    "        # tensor is [batch_size x max_length]\n",
    "        # where batch_size is the number of documents in this batch,\n",
    "        # and max_length is the maximal length of a document in the batch.\n",
    "\n",
    "        # We first look up the embeddings for all the words in the documents.\n",
    "        # The shape is now  [batch_size x max_length x embedding_dim]\n",
    "        embedded = self._embedding_layer.forward(sentences)\n",
    "\n",
    "        # We pack the padded sequence\n",
    "        # This is done for performance reasons and to ease the RNN training\n",
    "        packed_embeddings = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        # We push the packed input to the RNN layer\n",
    "        # The shape of the RNN output tensor is now [batch_size x max_length x rnn_output_size]\n",
    "        out_packed, _ = self._rnn_layer.forward(packed_embeddings)\n",
    "        \n",
    "        # We revert the packed output to a format similar to the original padded one\n",
    "        rnn_out, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        \n",
    "        # We then apply the linear output layer\n",
    "        # The shape of the output tensor is [batch_size x max_length x output_dim]\n",
    "        out = self._fc_layer.forward(rnn_out)\n",
    "        \n",
    "        # We can end the forward pass here. \n",
    "        # However, in order to ease the loss calculation, \n",
    "        # we configure the probability for the pad tokens manually so that the process progresses quicker\n",
    "        \n",
    "        # Find the positions where the token is a dummy padding token.\n",
    "        pad_mask = (sentences == self._pad_word_idx).float()\n",
    "\n",
    "        # For these positions, we add some large number in the column corresponding\n",
    "        # to the dummy padding label.\n",
    "        out[:, :, self._pad_label_idx] += pad_mask*10000\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def predict(self, sentences, lengths):\n",
    "        # Compute the outputs from the linear units.\n",
    "        \n",
    "        # scores is [batch_size x max_length x output_dim]\n",
    "        scores = self.forward(sentences, lengths)\n",
    "        \n",
    "        # Select the top-scoring labels\n",
    "        # predicted is [batch_size x max_length]\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        # We convert the prediction to a NumPy matrix.\n",
    "        return scores, predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training function below. In addition to that, we define helper functions about printing the statistics during evaluation and calculating the loss given the defined criterion and the predictions versus ground truth labels.\n",
    "\n",
    "As you can see, the training process is very similar to what we used for simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(criterion, scores, labels):\n",
    "    # We must flatten the scores into [(batch_size*max_length) x output_dim]\n",
    "    scores = scores.view(-1, scores.shape[2])\n",
    "    \n",
    "    # Same goes for labels. They now have a shape of [(batch_size*max_length)]\n",
    "    labels = labels.view(-1) \n",
    "    loss = criterion.forward(scores, labels) / scores.shape[0]\n",
    "    return loss\n",
    "\n",
    "def print_evaluation_stats(stats):\n",
    "    print()\n",
    "    print('Final evaluation on the validation set:')\n",
    "    p, r, f1 = prf(stats['total'])\n",
    "    print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "    for label in stats:\n",
    "        if label != 'total':\n",
    "            p, r, f1 = prf(stats[label])\n",
    "            print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    "    label_vocab,\n",
    "    pad_idx=1,\n",
    "    n_epochs=15):\n",
    "    \n",
    "    # Adam optimizer that will update the parameters of our model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "    \n",
    "    # Loss function that we will use during training\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        loss_sum = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (text, labels, lengths) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Print the progress during training\n",
    "            print(f'{i}/{len(train_dataloader)}                           \\r', end='')\n",
    "            \n",
    "            # Compute the output and loss.\n",
    "            scores = model.forward(text, lengths)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = calculate_loss(criterion, scores, labels)\n",
    "            \n",
    "            # Zero the gradients before performing the backward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform the backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Make a step in the parameter space\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save the loss value for future references\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        # Calculate the average loss for the full epoch and save it to the history\n",
    "        train_loss = loss_sum / len(train_dataloader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # Evaluate on the validation set.\n",
    "        stats = defaultdict(Counter)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_sum = 0\n",
    "            for (text, labels, lengths) in valid_dataloader:\n",
    "                # Predict the model's output on a batch.\n",
    "                scores, predicted = model.predict(text, lengths)\n",
    "                \n",
    "                # Calculate the loss and store it\n",
    "                loss = calculate_loss(criterion, scores, labels)\n",
    "                val_loss_sum += loss.item()\n",
    "                \n",
    "                # Update the evaluation statistics.\n",
    "                # This function modifies the stats object\n",
    "                evaluate_iob(predicted, labels, label_vocab, stats)\n",
    "\n",
    "        # Compute the overall F1 score for the validation set\n",
    "        _, _, val_f1 = prf(stats['total'])\n",
    "\n",
    "        # Save the validation results to the history\n",
    "        history['val_f1'].append(val_f1)\n",
    "        val_loss = val_loss_sum / len(valid_dataloader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        # Print some information about the epoch\n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {epoch}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "    # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "    # precision, recall, and F-scores for the different types of named entities.\n",
    "    print_evaluation_stats(stats)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now initialize our `RNNTagger` and finally train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNTagger(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    output_dim=len(label_vocab),\n",
    "    pad_word_idx=1,\n",
    "    pad_label_idx=1,\n",
    "    bidirectional=True,\n",
    "    n_layers=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_history = train(rnn_model, train_dataloader, valid_dataloader, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, let's see how the loss progressed during the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_stats(train_history):\n",
    "    plt.plot(train_history['train_loss'])\n",
    "    plt.plot(train_history['val_loss'])\n",
    "    plt.legend(['training loss', 'validation loss'])\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_stats(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the F1 score of the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_stats(train_history):\n",
    "    plt.plot(train_history['val_f1'])\n",
    "    plt.legend(['validation F1 score'])\n",
    "    plt.show()\n",
    "\n",
    "plot_f1_stats(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems in order. But we can never be too certain unless we try the model on unseen data. Let's define some functions that will help us do that.\n",
    "\n",
    "Firstly, we define the `create_evaluation_dataloader` helper function which builds a dataloader from raw sentences (represented as lists of list of tokens).\n",
    "\n",
    "Secondly, we have the `evaluate_sentences` which takes the tokens and the model, iterates over the data and predicts the labels. It then returns a collection of pairs - a token and its corresponding predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataloader(sentences, vocab, label_vocab, device):    \n",
    "    # First, create a torchtext Dataset containing the sentences to tag.\n",
    "    examples = []\n",
    "    for sen in sentences:\n",
    "        labels = ['?']*len(sen) # placeholder\n",
    "        examples.append((sen, labels))\n",
    "    \n",
    "    \n",
    "    tokens = convert_examples_to_tokens(examples, vocab, label_vocab)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        tokens,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=lambda x: collate_batch(x))\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def evaluate_sentences(sentences, model, vocab, label_vocab, device):\n",
    "    # This method applies the trained model to a list of sentences.\n",
    "    dataloader = create_evaluation_dataloader(sentences, vocab, label_vocab, device)\n",
    "\n",
    "    # Apply the trained model to all batches.\n",
    "    out = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (text, _, lengths) in dataloader:\n",
    "            # Call the model's predict method. This returns a list of NumPy matrix\n",
    "            # containing the integer-encoded tags for each sentence.\n",
    "            _, predicted = model.predict(text, lengths)\n",
    "\n",
    "            # Convert the integer-encoded tags to tag strings.\n",
    "            for tokens, pred_sen in zip(sentences, predicted):\n",
    "                out.append([\n",
    "                    (token, label_vocab.itos[pred_id])\n",
    "                    for token, pred_id in zip(tokens, pred_sen)\n",
    "                ])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the `print_tags` function which wraps what we just defined and prints the predictions in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tags(model, vocab, label_vocab, device, sentence):\n",
    "    tokens = sentence.split()\n",
    "    token_tags = evaluate_sentences([tokens], model, vocab, label_vocab, device)[0]\n",
    "    for token, tag in token_tags:\n",
    "        print(f'{token:12s}{tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tags(\n",
    "    rnn_model,\n",
    "    vocab,\n",
    "    label_vocab,\n",
    "    device,\n",
    "    'John Johnson was born in Moscow , lives in Gothenburg , and works for Chalmers \\\n",
    "    Technical University and the University of Gothenburg .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as if the model is capable to predict the entities correctly. As a final test, let's run the model over our test dataset and plot a confusion matrix for the results. Confusion matrices, also known as error matrices, allow us to better visualise our algorithm and to see if there are many mismatched entities. For example, we could see that the Person entity is often misrepresented as Location or similar.\n",
    "\n",
    "A perfect scenario would be a confusion matrix that has only its diagonal lit up. This would mean that X entity is predicted as X always and no confusions are occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(model, test_dataloader):\n",
    "    test_labels = []\n",
    "    test_predictions = []\n",
    "\n",
    "    for (text, labels, lengths) in test_dataloader:\n",
    "        _, predictions = model.predict(text, lengths)\n",
    "        test_labels.extend(labels.reshape(-1).detach().cpu().tolist())\n",
    "        test_predictions.extend(predictions.reshape(-1).tolist())\n",
    "\n",
    "    test_labels = [label_vocab.itos[x] for x in test_labels]\n",
    "    test_predictions = [label_vocab.itos[x] for x in test_predictions]\n",
    "    \n",
    "    return (test_labels, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(test_labels, test_predictions, label_vocab):\n",
    "    # we take all label strings\n",
    "    labels = label_vocab.itos\n",
    "    \n",
    "    # calculate the confusion matrix and normalize it\n",
    "    cm = metrics.confusion_matrix(test_labels, test_predictions, labels=labels)\n",
    "    cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1) # we add 1 to avoid division by zero\n",
    "\n",
    "    # plot the confusion matrix using seaborn heatmap\n",
    "    sns_heatmap = sns.heatmap(\n",
    "        cm,\n",
    "        cmap='RdYlGn_r',\n",
    "        square=True,\n",
    "        vmin=0,\n",
    "        vmax=1)\n",
    "\n",
    "    # beautify the plot so that it has better readability\n",
    "    plt.ylim(0, len(labels) + 0.5)\n",
    "    plt.ylim(0, len(labels) + 0.5)\n",
    "    plt.xlabel('Predicted values', labelpad=20)\n",
    "    plt.ylabel('True values')\n",
    "\n",
    "    sns_heatmap.set_yticklabels(labels, rotation=0)\n",
    "    sns_heatmap.set_xticklabels(labels, rotation=45, horizontalalignment='right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, test_predictions = evaluate_on_test(rnn_model, test_dataloader)\n",
    "plot_confusion_matrix(test_labels, test_predictions, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from our confusion matrix, the 'O' entity is the one causing most problems. This is to be expected, given its high occurrence in our data. We also see that there is still room for improvement, so let's try just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a Gated Recurrent Unit (GRU) using our previously defined `RNNTagger` model. The only difference is that we are passing the `rnn_type` argument with a value of 'gru'. \n",
    "\n",
    "Let's train the model and observe if there is any difference to the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = RNNTagger(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    output_dim=len(label_vocab),\n",
    "    pad_word_idx=1,\n",
    "    pad_label_idx=1,\n",
    "    bidirectional=True,\n",
    "    n_layers=1,\n",
    "    rnn_type='gru').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_history = train(gru_model, train_dataloader, valid_dataloader, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our training stats, similar to our previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_stats(gru_history)\n",
    "plot_f1_stats(gru_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these look good again, we need to be able to compare them with the previous ones. Below, we have a function that plots the f1 score history of our models together so that we can seee the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(history_results, result_labels):\n",
    "    for result in history_results:\n",
    "        plt.plot(result['val_f1'])\n",
    "        \n",
    "    plt.legend(result_labels)\n",
    "    plt.show()\n",
    "    \n",
    "compare_results([train_history, gru_history], ['RNN', 'GRU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that GRU outperforms the RNN model. This is to be expected and is the reason to why many scientists stick to it when working with recurrent networks. \n",
    "\n",
    "As a final check, let's see the confusion matrix of our GRU model too. \n",
    "\n",
    "Given the comparisons we saw, it should look better. Is this true? What is better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_test_labels, gru_test_predictions = evaluate_on_test(gru_model, test_dataloader)\n",
    "plot_confusion_matrix(gru_test_labels, gru_test_predictions, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Pre-trained embeddings\n",
    "\n",
    "Try to make use of the optional argument `pretrained_weights` in the `RNNTagger` module. You must define new vocabulary that makes use of the pretrained weights and then pass these to the model. \n",
    "\n",
    "_Hint: You can use some of the more popular ones like GloVe or FastText_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameters\n",
    "\n",
    "The `RNNTagger` module accepts many initialization parameters, also called hyper-parameters. Try to play with them, read what they do in the PyTorch documentation for the respective module and report your findings. Are any of those parameters more important than the rest or lead to much better or worse performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: RNN types\n",
    "\n",
    "As you can see, it is fairly easy to swap between different types of recurrent neural networks. As an exercise, try to implement other types than the ones shown here. One of the most popular ones would be the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n",
    "\n",
    "Once you have implemented it, report the results. Does it perform better or worse? What about the speed of training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Model checkpoints\n",
    "\n",
    "As you can see from our original F1 score charts, both models have peaks that occur before the end of the training. However, as we continue training, we overwrite the parameters and weights that produced these \"best\" results. In real scenarios, we must store them somewhere and at the end of the training, resume the model state to the one that produced best results. \n",
    "\n",
    "Create another `train()` function and modify it in a way that when validating, you make sure to save the model if a new best result is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Transformers\n",
    "\n",
    "RNN are getting increasingly outperformed in the recent years by another type of models, namely the Transformer networks. There are many ways a Transformer network can be applied to NER. It can be ideally replace the whole model (`RNNTagger` in our example) and predict the entity tags on its own. However, due to the nature of these networks, they are extremely large and require tremendous amount of time to train. Due to this, most people nowadays use them in a feature extraction type of manner, by replacing the embeddings in one network with the transformer model. This can be further trained during the NER task which is then called \"fine-tuning\" of the parameters.\n",
    "\n",
    "One of the most popular library for Transformers in Python is [HuggingFace](https://huggingface.co/). Try to implement  BERT - one of the most popular transformer models by following the library documentation. \n",
    "\n",
    "_Note that this requires higher amount of memory and GPU/CPU computation power. If you start getting memory errors, try reducing the size of batches and the model parameters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-env",
   "language": "python",
   "name": "lab-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
